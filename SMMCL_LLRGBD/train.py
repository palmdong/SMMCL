import os
import sys
import time
import argparse
from tqdm import tqdm
from tensorboardX import SummaryWriter  

import torch
import torch.nn as nn 
import torch.backends.cudnn as cudnn
from torch.nn.parallel import DistributedDataParallel

from config import config    
from dataloader.dataloader import get_train_loader
from dataloader.RGBXDataset import RGBXDataset
from models.builder import EncoderDecoder as segmodel    
from utils.init_func import group_weight  
from utils.lr_policy import WarmUpPolyLR
from utils.pyt_utils import all_reduce_tensor
from engine.engine import Engine
from engine.logger import get_logger


parser = argparse.ArgumentParser()  
logger = get_logger()               

os.environ['MASTER_PORT'] = '169710'


with Engine(custom_parser=parser) as engine:
    args = parser.parse_args()

    cudnn.benchmark = True
    seed = config.seed
    if engine.distributed:          
        seed = engine.local_rank
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)

    # Dataloader
    train_loader, train_sampler = get_train_loader(engine, RGBXDataset)  

    if (engine.distributed and (engine.local_rank == 0)) or (not engine.distributed):
        tb_dir = config.tb_dir + '/{}'.format(time.strftime("%b%d_%d-%H-%M", time.localtime()))
        generate_tb_dir = config.tb_dir + '/tb'
        tb = SummaryWriter(log_dir=tb_dir)          
        engine.link_tb(tb_dir, generate_tb_dir)     

    # Config Model
    criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=config.background) 

    if engine.distributed:      
        BatchNorm2d = nn.SyncBatchNorm
    else:
        BatchNorm2d = nn.BatchNorm2d    
    
    model=segmodel(cfg=config, criterion=criterion, norm_layer=BatchNorm2d)
    
    # Group Weight and Config Optimizer
    base_lr = config.lr
    if engine.distributed:     
        base_lr = config.lr
    
    params_list = []
    params_list = group_weight(params_list, model, BatchNorm2d, base_lr)  
    
    if config.optimizer == 'AdamW':
        optimizer = torch.optim.AdamW(params_list, lr=base_lr, betas=(0.9, 0.999), weight_decay=config.weight_decay)
    elif config.optimizer == 'SGDM':
        optimizer = torch.optim.SGD(params_list, lr=base_lr, momentum=config.momentum, weight_decay=config.weight_decay)
    else:
        raise NotImplementedError

    # Config LR Policy
    total_iteration = config.nepochs * config.niters_per_epoch  
    lr_policy = WarmUpPolyLR(base_lr, config.lr_power, total_iteration, config.niters_per_epoch * config.warm_up_epoch)  

    # Start Training 
    if engine.distributed:   
        logger.info('.............Distributed Training.............')  
        if torch.cuda.is_available():
            model.cuda()
            model = DistributedDataParallel(model, device_ids=[engine.local_rank], 
                                            output_device=engine.local_rank, find_unused_parameters=False)
    else:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        model = nn.DataParallel(model, device_ids=engine.devices) 

    engine.register_state(dataloader=train_loader, model=model,
                          optimizer=optimizer)                   
    if engine.continue_state_object:
        engine.restore_checkpoint()                              

    model.train()
    logger.info('Start Training:') 
    
    for epoch in range(engine.state.epoch, config.nepochs+1):
        if engine.distributed:                                   
            train_sampler.set_epoch(epoch)

        bar_format = '{desc}[{elapsed}<{remaining},{rate_fmt}]' 
        pbar = tqdm(range(config.niters_per_epoch), file=sys.stdout,
                    bar_format=bar_format)

        dataloader = iter(train_loader)
        sum_loss = 0

        for idx in pbar:   
            optimizer.zero_grad()           
            engine.update_iteration(epoch, idx)     

            minibatch = dataloader.next() 
            imgs = minibatch['data']       
            gts = minibatch['label']
            modal_xs = minibatch['modal_x']

            imgs = imgs.cuda(non_blocking=True)
            gts = gts.cuda(non_blocking=True)
            modal_xs = modal_xs.cuda(non_blocking=True)
                      
            loss = model(imgs, modal_xs, gts)    
            loss = loss.mean()                   

            # Reduce Loss over Multiple GPUs 
            if engine.distributed:                       
                reduce_loss = all_reduce_tensor(loss, world_size=engine.world_size)
            
            loss.backward()                 
            optimizer.step()              

            current_idx = (epoch- 1) * config.niters_per_epoch + idx
            lr = lr_policy.get_lr(current_idx)    

            for i in range(len(optimizer.param_groups)):
                optimizer.param_groups[i]['lr'] = lr

            if engine.distributed:
                sum_loss += reduce_loss.item()         
                print_str = 'Epoch {}/{}'.format(epoch, config.nepochs) \
                        + ' Iter {}/{}:'.format(idx + 1, config.niters_per_epoch) \
                        + ' lr=%.4e' % lr \
                        + ' loss=%.4f total_loss=%.4f' % (reduce_loss.item(), (sum_loss / (idx + 1)))
            else:
                sum_loss += loss
                print_str = 'Epoch {}/{}'.format(epoch, config.nepochs) \
                        + ' Iter {}/{}:'.format(idx + 1, config.niters_per_epoch) \
                        + ' lr=%.4e' % lr \
                        + ' loss=%.4f total_loss=%.4f' % (loss, (sum_loss / (idx + 1)))

            del loss
            pbar.set_description(print_str, refresh=False)
        
        if (engine.distributed and (engine.local_rank == 0)) or (not engine.distributed):      
            tb.add_scalar('train_loss', sum_loss / len(pbar), epoch)   

        if (epoch >= config.checkpoint_start_epoch) and (epoch % config.checkpoint_step == 0) or (epoch == config.nepochs):
            if engine.distributed and (engine.local_rank == 0):         
                engine.save_and_link_checkpoint(config.checkpoint_dir,
                                                config.log_dir,
                                                config.log_dir_link)
            elif not engine.distributed:
                engine.save_and_link_checkpoint(config.checkpoint_dir,
                                                config.log_dir,
                                                config.log_dir_link)
